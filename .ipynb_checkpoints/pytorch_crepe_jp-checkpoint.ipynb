{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Creating vocab...\n",
      "Build model...\n",
      "CharCNN (\n",
      "  (features): Sequential (\n",
      "    (0): Conv1d(69, 256, kernel_size=(7,), stride=(1,))\n",
      "    (1): ReLU ()\n",
      "    (2): MaxPool1d (size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv1d(256, 256, kernel_size=(7,), stride=(1,))\n",
      "    (4): ReLU ()\n",
      "    (5): MaxPool1d (size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
      "    (7): ReLU ()\n",
      "    (8): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
      "    (9): ReLU ()\n",
      "    (10): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
      "    (11): ReLU ()\n",
      "    (12): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
      "    (13): ReLU ()\n",
      "    (14): MaxPool1d (size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential (\n",
      "    (0): Dropout (p = 0.5)\n",
      "    (1): Linear (1280 -> 1024)\n",
      "    (2): ReLU (inplace)\n",
      "    (3): Dropout (p = 0.5)\n",
      "    (4): Linear (1024 -> 1024)\n",
      "    (5): ReLU (inplace)\n",
      "    (6): Linear (1024 -> 4)\n",
      "  )\n",
      ")\n",
      "train model...\n"
     ]
    }
   ],
   "source": [
    "# Import all libraries\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pytorch_crepe\n",
    "import datetime\n",
    "import numpy as np\n",
    "import data_helpers\n",
    "from CharCNN import CharCNN\n",
    "np.random.seed(0123)\n",
    "\n",
    "\n",
    "# set parameters:\n",
    "\n",
    "subset = None\n",
    "\n",
    "#Whether to save model parameters\n",
    "save = False\n",
    "model_name_path = 'params/crepe_model.json'\n",
    "model_weights_path = 'params/crepe_model_weights.h5'\n",
    "\n",
    "\n",
    "#Compile/fit params\n",
    "batch_size = 50\n",
    "nb_epoch = 10\n",
    "maxlen = 256\n",
    "\n",
    "\n",
    "\n",
    "print('Loading data...')\n",
    "#Expect x to be a list of sentences. Y to be a one-hot encoding of the\n",
    "#categories.\n",
    "(xt, yt), (x_test, y_test) = data_helpers.load_ag_data()\n",
    "\n",
    "print('Creating vocab...')\n",
    "vocab, reverse_vocab, vocab_size, check = data_helpers.create_vocab_set()\n",
    "test_data = data_helpers.encode_data(x_test, maxlen, vocab, vocab_size, check)\n",
    "\n",
    "\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "model = CharCNN()\n",
    "model.cuda()\n",
    "print(model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01,momentum=0.9)\n",
    "\n",
    "\n",
    "\n",
    "print('train model...')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "  Step: 100\n",
      "\tLoss: 1.38647389412. Accuracy: 24.44\n",
      "  Step: 200\n",
      "\tLoss: 1.38630878925. Accuracy: 24.81\n",
      "  Step: 300\n",
      "\tLoss: 1.3844435215. Accuracy: 25.16\n",
      "  Step: 400\n",
      "\tLoss: 1.38384795189. Accuracy: 25.16\n",
      "  Step: 500\n",
      "\tLoss: 1.38461732864. Accuracy: 25.272\n",
      "  Step: 600\n",
      "\tLoss: 1.38668644428. Accuracy: 25.0966666667\n",
      "  Step: 700\n",
      "\tLoss: 1.382396698. Accuracy: 25.0542857143\n",
      "  Step: 800\n",
      "\tLoss: 1.39715886116. Accuracy: 25.085\n",
      "  Step: 900\n",
      "\tLoss: 1.37974023819. Accuracy: 25.0977777778\n",
      "  Step: 1000\n",
      "\tLoss: 1.38750219345. Accuracy: 25.048\n",
      "  Step: 1100\n",
      "\tLoss: 1.37734007835. Accuracy: 25.0309090909\n",
      "  Step: 1200\n",
      "\tLoss: 1.39204728603. Accuracy: 25.0\n",
      "  Step: 1300\n",
      "\tLoss: 1.3854572773. Accuracy: 25.0030769231\n",
      "  Step: 1400\n",
      "\tLoss: 1.37769877911. Accuracy: 25.0714285714\n",
      "  Step: 1500\n",
      "\tLoss: 1.38696491718. Accuracy: 25.0013333333\n",
      "  Step: 1600\n",
      "\tLoss: 1.38585817814. Accuracy: 24.9675\n",
      "  Step: 1700\n",
      "\tLoss: 1.38086080551. Accuracy: 24.9917647059\n",
      "  Step: 1800\n",
      "\tLoss: 1.3869562149. Accuracy: 24.9788888889\n",
      "  Step: 1900\n",
      "\tLoss: 1.38147211075. Accuracy: 24.9957894737\n",
      "  Step: 2000\n",
      "\tLoss: 1.38738918304. Accuracy: 24.96\n",
      "  Step: 2100\n",
      "\tLoss: 1.38207626343. Accuracy: 24.9495238095\n",
      "  Step: 2200\n",
      "\tLoss: 1.39704668522. Accuracy: 24.9481818182\n",
      "  Step: 2300\n",
      "\tLoss: 1.38600993156. Accuracy: 24.9504347826\n",
      "  Step: 2400\n",
      "\tLoss: 1.38484025002. Accuracy: 24.9341666667\n",
      "Test Loss: 10536.9993874. Test Accuracy: 190000.0\n",
      "Epoch: 1\n",
      "  Step: 100\n",
      "\tLoss: 1.37329816818. Accuracy: 26.1\n",
      "  Step: 200\n",
      "\tLoss: 1.38593006134. Accuracy: 25.74\n",
      "  Step: 300\n",
      "\tLoss: 1.38682615757. Accuracy: 25.46\n",
      "  Step: 400\n",
      "\tLoss: 1.38446760178. Accuracy: 25.395\n",
      "  Step: 500\n",
      "\tLoss: 1.37900388241. Accuracy: 25.3\n",
      "  Step: 600\n",
      "\tLoss: 1.38670241833. Accuracy: 25.1933333333\n",
      "  Step: 700\n",
      "\tLoss: 1.38466656208. Accuracy: 25.14\n",
      "  Step: 800\n",
      "\tLoss: 1.38315057755. Accuracy: 25.03\n",
      "  Step: 900\n",
      "\tLoss: 1.38569116592. Accuracy: 25.08\n",
      "  Step: 1000\n",
      "\tLoss: 1.3796826601. Accuracy: 25.09\n",
      "  Step: 1100\n",
      "\tLoss: 1.39260029793. Accuracy: 25.0854545455\n",
      "  Step: 1200\n",
      "\tLoss: 1.38877475262. Accuracy: 25.1566666667\n",
      "  Step: 1300\n",
      "\tLoss: 1.3883074522. Accuracy: 25.1230769231\n",
      "  Step: 1400\n",
      "\tLoss: 1.38999223709. Accuracy: 25.1257142857\n",
      "  Step: 1500\n",
      "\tLoss: 1.38879549503. Accuracy: 25.148\n",
      "  Step: 1600\n",
      "\tLoss: 1.37672650814. Accuracy: 25.13625\n",
      "  Step: 1700\n",
      "\tLoss: 1.38450074196. Accuracy: 25.1423529412\n",
      "  Step: 1800\n",
      "\tLoss: 1.38516330719. Accuracy: 25.15\n",
      "  Step: 1900\n",
      "\tLoss: 1.39206898212. Accuracy: 25.1389473684\n",
      "  Step: 2000\n",
      "\tLoss: 1.38737797737. Accuracy: 25.152\n",
      "  Step: 2100\n",
      "\tLoss: 1.38337695599. Accuracy: 25.1495238095\n",
      "  Step: 2200\n",
      "\tLoss: 1.38953888416. Accuracy: 25.13\n",
      "  Step: 2300\n",
      "\tLoss: 1.39252436161. Accuracy: 25.1608695652\n",
      "  Step: 2400\n",
      "\tLoss: 1.38492977619. Accuracy: 25.1708333333\n"
     ]
    }
   ],
   "source": [
    " for e in xrange(nb_epoch):\n",
    "    xi, yi = data_helpers.shuffle_matrix(xt, yt)\n",
    "    xi_test, yi_test = data_helpers.shuffle_matrix(x_test, y_test)\n",
    "    if subset:\n",
    "        batches = data_helpers.mini_batch_generator(xi[:subset], yi[:subset],\n",
    "                                                    vocab, vocab_size, check,\n",
    "                                                    maxlen,\n",
    "                                                    batch_size=batch_size)\n",
    "    else:\n",
    "        batches = data_helpers.mini_batch_generator(xi, yi, vocab, vocab_size,\n",
    "                                                    check, maxlen,\n",
    "                                                    batch_size=batch_size)\n",
    "\n",
    "    test_batches = data_helpers.mini_batch_generator(xi_test, yi_test, vocab,\n",
    "                                                     vocab_size, check, maxlen,\n",
    "                                                     batch_size=1)\n",
    "\n",
    "    accuracy = 0.0\n",
    "   \n",
    "    step = 1\n",
    "    start = datetime.datetime.now()\n",
    "    print('Epoch: {}'.format(e))\n",
    "    running_loss = 0.0\n",
    "    i=0\n",
    "    train_loss_avg=0.0\n",
    "    train_correct=0.0\n",
    "    for x_train, y_train in batches:\n",
    "        i=i+1\n",
    "        inputs=torch.from_numpy(np.swapaxes(x_train.astype(np.float64),1,2))\n",
    "        labels=torch.from_numpy(np.argmax(y_train, axis=1).astype(np.float64))\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "        inputs=inputs.float()\n",
    "        labels=labels.long()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #train_loss += loss.data[0]\n",
    "        train_loss_avg =loss.data.mean()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_correct = (predicted == labels.data).sum()\n",
    "        accuracy +=  (train_correct*100/len(labels))\n",
    "        accuracy_avg = accuracy / step\n",
    "        if step % 100 == 0:\n",
    "            print('  Step: {}'.format(step))\n",
    "            print('\\tLoss: {}. Accuracy: {}'.format(train_loss_avg,accuracy_avg))\n",
    "        step += 1\n",
    "    test_accuracy=0\n",
    "    test_loss_avg=0\n",
    "    test_correct=0\n",
    "    step=1\n",
    "    test_accuracy_avg=0\n",
    "    for x_test_batch, y_test_batch in test_batches:\n",
    "        inputs=torch.from_numpy(np.swapaxes(x_test_batch.astype(np.float64),1,2))\n",
    "        labels=torch.from_numpy(np.argmax(y_test_batch, axis=1).astype(np.float64))\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "        inputs=inputs.float()\n",
    "        labels=labels.long()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss_avg += loss.data[0]\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_correct = (predicted == labels.data).sum()\n",
    "        test_accuracy +=  (test_correct*100/len(labels))\n",
    "        #test_accuracy_avg += (test_accuracy /step)\n",
    "        step+=1\n",
    "    print('Test Loss: {}. Test Accuracy: {}'.format(test_loss_avg/step,test_accuracy/step))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
