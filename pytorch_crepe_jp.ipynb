{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Creating vocab...\n",
      "Build model...\n",
      "CharCNN (\n",
      "  (features): Sequential (\n",
      "    (0): Conv1d(69, 256, kernel_size=(7,), stride=(1,))\n",
      "    (1): ReLU ()\n",
      "    (2): MaxPool1d (size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv1d(256, 256, kernel_size=(7,), stride=(1,))\n",
      "    (4): ReLU ()\n",
      "    (5): MaxPool1d (size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
      "    (7): ReLU ()\n",
      "    (8): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
      "    (9): ReLU ()\n",
      "    (10): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
      "    (11): ReLU ()\n",
      "    (12): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
      "    (13): ReLU ()\n",
      "    (14): MaxPool1d (size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential (\n",
      "    (0): Dropout (p = 0.5)\n",
      "    (1): Linear (1280 -> 1024)\n",
      "    (2): ReLU (inplace)\n",
      "    (3): Dropout (p = 0.5)\n",
      "    (4): Linear (1024 -> 1024)\n",
      "    (5): ReLU (inplace)\n",
      "    (6): Linear (1024 -> 4)\n",
      "  )\n",
      ")\n",
      "train model...\n"
     ]
    }
   ],
   "source": [
    "# Import all libraries\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pytorch_crepe\n",
    "import datetime\n",
    "import numpy as np\n",
    "import data_helpers\n",
    "from CharCNN import CharCNN\n",
    "np.random.seed(0123)\n",
    "\n",
    "\n",
    "# set parameters:\n",
    "\n",
    "subset = None\n",
    "\n",
    "#Whether to save model parameters\n",
    "save = False\n",
    "model_name_path = 'params/crepe_model.json'\n",
    "model_weights_path = 'params/crepe_model_weights.h5'\n",
    "\n",
    "\n",
    "#Compile/fit params\n",
    "batch_size = 50\n",
    "nb_epoch = 10\n",
    "maxlen = 256\n",
    "\n",
    "\n",
    "\n",
    "print('Loading data...')\n",
    "#Expect x to be a list of sentences. Y to be a one-hot encoding of the\n",
    "#categories.\n",
    "(xt, yt), (x_test, y_test) = data_helpers.load_ag_data()\n",
    "\n",
    "print('Creating vocab...')\n",
    "vocab, reverse_vocab, vocab_size, check = data_helpers.create_vocab_set()\n",
    "test_data = data_helpers.encode_data(x_test, maxlen, vocab, vocab_size, check)\n",
    "\n",
    "\n",
    "\n",
    "print('Build model...')\n",
    "\n",
    "model = CharCNN()\n",
    "model.cuda()\n",
    "print(model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01,momentum=0.9)\n",
    "\n",
    "\n",
    "\n",
    "print('train model...')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "  Step: 100\n",
      "\tLoss: 1.38683974743. Accuracy: 24.74\n",
      "  Step: 200\n",
      "\tLoss: 1.37997865677. Accuracy: 24.85\n",
      "  Step: 300\n",
      "\tLoss: 1.38698852062. Accuracy: 24.9666666667\n",
      "  Step: 400\n",
      "\tLoss: 1.38279271126. Accuracy: 24.905\n",
      "  Step: 500\n",
      "\tLoss: 1.38365769386. Accuracy: 24.688\n",
      "  Step: 600\n",
      "\tLoss: 1.3818192482. Accuracy: 24.9233333333\n",
      "  Step: 700\n",
      "\tLoss: 1.39088129997. Accuracy: 24.96\n",
      "  Step: 800\n",
      "\tLoss: 1.39429712296. Accuracy: 24.915\n",
      "  Step: 900\n",
      "\tLoss: 1.38044857979. Accuracy: 24.9711111111\n",
      "  Step: 1000\n",
      "\tLoss: 1.38008916378. Accuracy: 24.98\n",
      "  Step: 1100\n",
      "\tLoss: 1.38725495338. Accuracy: 24.9054545455\n",
      "  Step: 1200\n",
      "\tLoss: 1.39281094074. Accuracy: 24.9583333333\n",
      "  Step: 1300\n",
      "\tLoss: 1.39067745209. Accuracy: 25.0015384615\n",
      "  Step: 1400\n",
      "\tLoss: 1.38153719902. Accuracy: 25.0014285714\n",
      "  Step: 1500\n",
      "\tLoss: 1.38250911236. Accuracy: 25.0333333333\n",
      "  Step: 1600\n",
      "\tLoss: 1.39005494118. Accuracy: 25.05\n",
      "  Step: 1700\n",
      "\tLoss: 1.38569116592. Accuracy: 25.0211764706\n",
      "  Step: 1800\n",
      "\tLoss: 1.38031184673. Accuracy: 24.9866666667\n",
      "  Step: 1900\n",
      "\tLoss: 1.38960385323. Accuracy: 24.96\n",
      "  Step: 2000\n",
      "\tLoss: 1.38490772247. Accuracy: 24.977\n",
      "  Step: 2100\n",
      "\tLoss: 1.38561403751. Accuracy: 24.9638095238\n",
      "  Step: 2200\n",
      "\tLoss: 1.38725709915. Accuracy: 24.9890909091\n",
      "  Step: 2300\n",
      "\tLoss: 1.38628101349. Accuracy: 24.9982608696\n",
      "  Step: 2400\n",
      "\tLoss: 1.38971436024. Accuracy: 24.99\n",
      "Test Loss: 1.38631894105. Test Accuracy: 25.4834890146\n",
      "Epoch: 1\n",
      "  Step: 100\n",
      "\tLoss: 1.38631296158. Accuracy: 24.9\n",
      "  Step: 200\n",
      "\tLoss: 1.38739049435. Accuracy: 25.05\n",
      "  Step: 300\n",
      "\tLoss: 1.38620316982. Accuracy: 25.08\n",
      "  Step: 400\n",
      "\tLoss: 1.38214504719. Accuracy: 25.47\n",
      "  Step: 500\n",
      "\tLoss: 1.38911366463. Accuracy: 25.372\n",
      "  Step: 600\n",
      "\tLoss: 1.38233613968. Accuracy: 25.33\n",
      "  Step: 700\n",
      "\tLoss: 1.3870433569. Accuracy: 25.1714285714\n",
      "  Step: 800\n",
      "\tLoss: 1.38668644428. Accuracy: 25.145\n",
      "  Step: 900\n",
      "\tLoss: 1.39136219025. Accuracy: 25.0733333333\n",
      "  Step: 1000\n",
      "\tLoss: 1.39441454411. Accuracy: 25.16\n",
      "  Step: 1100\n",
      "\tLoss: 1.39011156559. Accuracy: 25.0054545455\n",
      "  Step: 1200\n",
      "\tLoss: 1.38491678238. Accuracy: 25.0116666667\n",
      "  Step: 1300\n",
      "\tLoss: 1.38709950447. Accuracy: 24.9892307692\n",
      "  Step: 1400\n",
      "\tLoss: 1.39016842842. Accuracy: 24.97\n",
      "  Step: 1500\n",
      "\tLoss: 1.38992643356. Accuracy: 25.0426666667\n",
      "  Step: 1600\n",
      "\tLoss: 1.39155042171. Accuracy: 24.9975\n",
      "  Step: 1700\n",
      "\tLoss: 1.38618147373. Accuracy: 24.9847058824\n",
      "  Step: 1800\n",
      "\tLoss: 1.37701702118. Accuracy: 25.0411111111\n",
      "  Step: 1900\n",
      "\tLoss: 1.38587152958. Accuracy: 24.9915789474\n",
      "  Step: 2000\n",
      "\tLoss: 1.3829780817. Accuracy: 25.007\n",
      "  Step: 2100\n",
      "\tLoss: 1.38658428192. Accuracy: 25.0133333333\n",
      "  Step: 2200\n",
      "\tLoss: 1.38583445549. Accuracy: 25.0072727273\n",
      "  Step: 2300\n",
      "\tLoss: 1.38938379288. Accuracy: 25.0356521739\n",
      "  Step: 2400\n",
      "\tLoss: 1.39041996002. Accuracy: 25.0508333333\n",
      "Test Loss: 1.38603649733. Test Accuracy: 24.9967109591\n",
      "Epoch: 2\n",
      "  Step: 100\n",
      "\tLoss: 1.39076161385. Accuracy: 25.48\n",
      "  Step: 200\n",
      "\tLoss: 1.38564300537. Accuracy: 25.63\n",
      "  Step: 300\n",
      "\tLoss: 1.38605773449. Accuracy: 25.3866666667\n",
      "  Step: 400\n",
      "\tLoss: 1.39502811432. Accuracy: 25.425\n",
      "  Step: 500\n",
      "\tLoss: 1.3871679306. Accuracy: 25.332\n",
      "  Step: 600\n",
      "\tLoss: 1.38611710072. Accuracy: 25.5666666667\n",
      "  Step: 700\n",
      "\tLoss: 1.38824939728. Accuracy: 25.5428571429\n",
      "  Step: 800\n",
      "\tLoss: 1.38772201538. Accuracy: 25.735\n",
      "  Step: 900\n",
      "\tLoss: 1.38671052456. Accuracy: 25.8222222222\n",
      "  Step: 1000\n",
      "\tLoss: 1.39158535004. Accuracy: 25.852\n",
      "  Step: 1100\n",
      "\tLoss: 1.37878489494. Accuracy: 25.9945454545\n",
      "  Step: 1200\n",
      "\tLoss: 1.37996017933. Accuracy: 26.27\n",
      "  Step: 1300\n",
      "\tLoss: 1.3805475235. Accuracy: 26.4923076923\n",
      "  Step: 1400\n",
      "\tLoss: 1.37269282341. Accuracy: 26.6642857143\n",
      "  Step: 1500\n",
      "\tLoss: 1.36681437492. Accuracy: 26.9133333333\n",
      "  Step: 1600\n",
      "\tLoss: 1.36596143246. Accuracy: 27.0675\n",
      "  Step: 1700\n",
      "\tLoss: 1.37665188313. Accuracy: 27.2211764706\n",
      "  Step: 1800\n",
      "\tLoss: 1.33013749123. Accuracy: 27.3177777778\n",
      "  Step: 1900\n",
      "\tLoss: 1.34253919125. Accuracy: 27.4789473684\n",
      "  Step: 2000\n",
      "\tLoss: 1.37114405632. Accuracy: 27.639\n",
      "  Step: 2100\n",
      "\tLoss: 1.4010206461. Accuracy: 27.8257142857\n",
      "  Step: 2200\n",
      "\tLoss: 1.33686101437. Accuracy: 28.0045454545\n",
      "  Step: 2300\n",
      "\tLoss: 1.35773181915. Accuracy: 28.187826087\n",
      "  Step: 2400\n",
      "\tLoss: 1.44502592087. Accuracy: 28.4066666667\n",
      "Test Loss: 1.40721923637. Test Accuracy: 29.0488093672\n",
      "Epoch: 3\n",
      "  Step: 100\n",
      "\tLoss: 1.31206226349. Accuracy: 33.8\n",
      "  Step: 200\n",
      "\tLoss: 1.22413146496. Accuracy: 34.65\n",
      "  Step: 300\n",
      "\tLoss: 1.30435597897. Accuracy: 36.1333333333\n",
      "  Step: 400\n",
      "\tLoss: 1.22888803482. Accuracy: 37.04\n",
      "  Step: 500\n",
      "\tLoss: 1.23035252094. Accuracy: 37.58\n",
      "  Step: 600\n",
      "\tLoss: 1.28905940056. Accuracy: 38.31\n",
      "  Step: 700\n",
      "\tLoss: 1.28790009022. Accuracy: 39.2485714286\n",
      "  Step: 800\n",
      "\tLoss: 1.13567113876. Accuracy: 39.675\n",
      "  Step: 900\n",
      "\tLoss: 1.10206234455. Accuracy: 40.2577777778\n",
      "  Step: 1000\n",
      "\tLoss: 1.10050857067. Accuracy: 41.06\n",
      "  Step: 1100\n",
      "\tLoss: 1.1496629715. Accuracy: 41.9654545455\n",
      "  Step: 1200\n",
      "\tLoss: 1.06611895561. Accuracy: 42.7466666667\n",
      "  Step: 1300\n",
      "\tLoss: 0.885872066021. Accuracy: 43.3523076923\n",
      "  Step: 1400\n",
      "\tLoss: 1.07468235493. Accuracy: 44.0871428571\n",
      "  Step: 1500\n",
      "\tLoss: 0.983625471592. Accuracy: 44.5893333333\n",
      "  Step: 1600\n",
      "\tLoss: 1.12289655209. Accuracy: 45.33\n",
      "  Step: 1700\n",
      "\tLoss: 0.906746685505. Accuracy: 46.0152941176\n",
      "  Step: 1800\n",
      "\tLoss: 0.90517771244. Accuracy: 46.5566666667\n",
      "  Step: 1900\n",
      "\tLoss: 0.852820038795. Accuracy: 47.1526315789\n",
      "  Step: 2000\n",
      "\tLoss: 0.890344083309. Accuracy: 47.713\n",
      "  Step: 2100\n",
      "\tLoss: 1.19001209736. Accuracy: 48.3504761905\n",
      "  Step: 2200\n",
      "\tLoss: 0.660803735256. Accuracy: 48.8436363636\n",
      "  Step: 2300\n",
      "\tLoss: 0.752204537392. Accuracy: 49.2382608696\n",
      "  Step: 2400\n",
      "\tLoss: 0.836905658245. Accuracy: 49.6325\n",
      "Test Loss: 0.942219115663. Test Accuracy: 56.9925009867\n",
      "Epoch: 4\n",
      "  Step: 100\n",
      "\tLoss: 0.897816300392. Accuracy: 61.5\n",
      "  Step: 200\n",
      "\tLoss: 0.750203549862. Accuracy: 62.27\n",
      "  Step: 300\n",
      "\tLoss: 0.771216452122. Accuracy: 62.52\n",
      "  Step: 400\n",
      "\tLoss: 0.898378372192. Accuracy: 62.43\n",
      "  Step: 500\n",
      "\tLoss: 0.910356760025. Accuracy: 62.492\n",
      "  Step: 600\n",
      "\tLoss: 0.741290509701. Accuracy: 62.7233333333\n",
      "  Step: 700\n",
      "\tLoss: 0.773463666439. Accuracy: 62.7885714286\n",
      "  Step: 800\n",
      "\tLoss: 0.87120360136. Accuracy: 62.7275\n",
      "  Step: 900\n",
      "\tLoss: 0.775691509247. Accuracy: 62.8622222222\n",
      "  Step: 1000\n",
      "\tLoss: 0.800753772259. Accuracy: 63.01\n",
      "  Step: 1100\n",
      "\tLoss: 0.565074145794. Accuracy: 63.3690909091\n",
      "  Step: 1200\n",
      "\tLoss: 0.845160245895. Accuracy: 63.6683333333\n",
      "  Step: 1300\n",
      "\tLoss: 0.663851618767. Accuracy: 63.8292307692\n",
      "  Step: 1400\n",
      "\tLoss: 0.969721734524. Accuracy: 63.9914285714\n",
      "  Step: 1500\n",
      "\tLoss: 0.582369983196. Accuracy: 64.272\n",
      "  Step: 1600\n",
      "\tLoss: 0.56146389246. Accuracy: 64.55375\n",
      "  Step: 1700\n",
      "\tLoss: 0.465313643217. Accuracy: 64.76\n",
      "  Step: 1800\n",
      "\tLoss: 0.673723042011. Accuracy: 64.9144444444\n",
      "  Step: 1900\n",
      "\tLoss: 0.665165841579. Accuracy: 65.1673684211\n",
      "  Step: 2000\n",
      "\tLoss: 0.758893966675. Accuracy: 65.42\n",
      "  Step: 2100\n",
      "\tLoss: 0.544735074043. Accuracy: 65.6133333333\n",
      "  Step: 2200\n",
      "\tLoss: 0.648204088211. Accuracy: 65.8836363636\n",
      "  Step: 2300\n",
      "\tLoss: 0.722042441368. Accuracy: 66.112173913\n",
      "  Step: 2400\n",
      "\tLoss: 0.498569518328. Accuracy: 66.425\n",
      "Test Loss: 0.691891017768. Test Accuracy: 71.5037495066\n",
      "Epoch: 5\n",
      "  Step: 100\n",
      "\tLoss: 0.469688385725. Accuracy: 73.04\n",
      "  Step: 200\n",
      "\tLoss: 0.672155678272. Accuracy: 73.35\n",
      "  Step: 300\n",
      "\tLoss: 0.639633953571. Accuracy: 74.0133333333\n",
      "  Step: 400\n",
      "\tLoss: 0.681473553181. Accuracy: 74.0\n",
      "  Step: 500\n",
      "\tLoss: 0.575194716454. Accuracy: 74.568\n",
      "  Step: 600\n",
      "\tLoss: 0.500448644161. Accuracy: 74.8466666667\n",
      "  Step: 700\n",
      "\tLoss: 0.693276286125. Accuracy: 75.0857142857\n",
      "  Step: 800\n",
      "\tLoss: 0.582871973515. Accuracy: 75.255\n",
      "  Step: 900\n",
      "\tLoss: 0.923662662506. Accuracy: 75.5644444444\n",
      "  Step: 1000\n",
      "\tLoss: 0.727115809917. Accuracy: 75.668\n",
      "  Step: 1100\n",
      "\tLoss: 0.455194085836. Accuracy: 75.7909090909\n",
      "  Step: 1200\n",
      "\tLoss: 0.564097762108. Accuracy: 76.0266666667\n",
      "  Step: 1300\n",
      "\tLoss: 0.399900972843. Accuracy: 76.1492307692\n",
      "  Step: 1400\n",
      "\tLoss: 0.372202455997. Accuracy: 76.3514285714\n",
      "  Step: 1500\n",
      "\tLoss: 0.459412753582. Accuracy: 76.544\n",
      "  Step: 1600\n",
      "\tLoss: 0.351758718491. Accuracy: 76.75375\n",
      "  Step: 1700\n",
      "\tLoss: 0.426515728235. Accuracy: 76.8482352941\n",
      "  Step: 1800\n",
      "\tLoss: 0.629810094833. Accuracy: 76.9511111111\n",
      "  Step: 1900\n",
      "\tLoss: 0.374878495932. Accuracy: 77.0684210526\n",
      "  Step: 2000\n",
      "\tLoss: 0.509241521358. Accuracy: 77.258\n",
      "  Step: 2100\n",
      "\tLoss: 0.520217299461. Accuracy: 77.3580952381\n",
      "  Step: 2200\n",
      "\tLoss: 0.456118881702. Accuracy: 77.5290909091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step: 2300\n",
      "\tLoss: 0.391992300749. Accuracy: 77.727826087\n",
      "  Step: 2400\n",
      "\tLoss: 0.351631015539. Accuracy: 77.87\n",
      "Test Loss: 0.540943343715. Test Accuracy: 80.2920668333\n",
      "Epoch: 6\n",
      "  Step: 100\n",
      "\tLoss: 0.345025032759. Accuracy: 82.8\n",
      "  Step: 200\n",
      "\tLoss: 0.715629935265. Accuracy: 83.14\n",
      "  Step: 300\n",
      "\tLoss: 0.237282812595. Accuracy: 83.02\n",
      "  Step: 400\n",
      "\tLoss: 0.516407251358. Accuracy: 82.915\n",
      "  Step: 500\n",
      "\tLoss: 0.363313317299. Accuracy: 82.808\n",
      "  Step: 600\n",
      "\tLoss: 0.513000488281. Accuracy: 83.0333333333\n",
      "  Step: 700\n",
      "\tLoss: 0.438504070044. Accuracy: 83.1257142857\n",
      "  Step: 800\n",
      "\tLoss: 0.57549059391. Accuracy: 83.31\n",
      "  Step: 900\n",
      "\tLoss: 0.359960317612. Accuracy: 83.4711111111\n",
      "  Step: 1000\n",
      "\tLoss: 0.566213905811. Accuracy: 83.524\n",
      "  Step: 1100\n",
      "\tLoss: 0.474657595158. Accuracy: 83.6436363636\n",
      "  Step: 1200\n",
      "\tLoss: 0.384576648474. Accuracy: 83.6566666667\n",
      "  Step: 1300\n",
      "\tLoss: 0.330178290606. Accuracy: 83.7415384615\n",
      "  Step: 1400\n",
      "\tLoss: 0.498018443584. Accuracy: 83.6842857143\n",
      "  Step: 1500\n",
      "\tLoss: 0.358170002699. Accuracy: 83.7506666667\n",
      "  Step: 1600\n",
      "\tLoss: 0.466876596212. Accuracy: 83.85875\n",
      "  Step: 1700\n",
      "\tLoss: 0.46491008997. Accuracy: 83.9035294118\n",
      "  Step: 1800\n",
      "\tLoss: 0.188335463405. Accuracy: 83.9011111111\n",
      "  Step: 1900\n",
      "\tLoss: 0.446355909109. Accuracy: 83.9884210526\n",
      "  Step: 2000\n",
      "\tLoss: 0.505836427212. Accuracy: 84.042\n",
      "  Step: 2100\n",
      "\tLoss: 0.736627578735. Accuracy: 84.1257142857\n",
      "  Step: 2200\n",
      "\tLoss: 0.383155673742. Accuracy: 84.2290909091\n",
      "  Step: 2300\n",
      "\tLoss: 0.368097692728. Accuracy: 84.2852173913\n",
      "  Step: 2400\n",
      "\tLoss: 0.321741402149. Accuracy: 84.29\n",
      "Test Loss: 0.42737689359. Test Accuracy: 84.9493487699\n",
      "Epoch: 7\n",
      "  Step: 100\n",
      "\tLoss: 0.36307656765. Accuracy: 86.92\n",
      "  Step: 200\n",
      "\tLoss: 0.225346371531. Accuracy: 86.79\n",
      "  Step: 300\n",
      "\tLoss: 0.386930078268. Accuracy: 87.0266666667\n",
      "  Step: 400\n",
      "\tLoss: 0.595779776573. Accuracy: 86.525\n",
      "  Step: 500\n",
      "\tLoss: 0.393278568983. Accuracy: 86.444\n",
      "  Step: 600\n",
      "\tLoss: 0.157425060868. Accuracy: 86.5633333333\n",
      "  Step: 700\n",
      "\tLoss: 0.381478577852. Accuracy: 86.5285714286\n",
      "  Step: 800\n",
      "\tLoss: 0.569497942924. Accuracy: 86.53\n",
      "  Step: 900\n",
      "\tLoss: 0.431442022324. Accuracy: 86.6044444444\n",
      "  Step: 1000\n",
      "\tLoss: 0.319329082966. Accuracy: 86.69\n",
      "  Step: 1100\n",
      "\tLoss: 0.248272299767. Accuracy: 86.7836363636\n",
      "  Step: 1200\n",
      "\tLoss: 0.340911448002. Accuracy: 86.815\n",
      "  Step: 1300\n",
      "\tLoss: 0.47854155302. Accuracy: 86.7661538462\n",
      "  Step: 1400\n",
      "\tLoss: 0.267839193344. Accuracy: 86.7242857143\n",
      "  Step: 1500\n",
      "\tLoss: 0.518135428429. Accuracy: 86.7333333333\n",
      "  Step: 1600\n",
      "\tLoss: 0.333041042089. Accuracy: 86.7\n",
      "  Step: 1700\n",
      "\tLoss: 0.301194608212. Accuracy: 86.7517647059\n",
      "  Step: 1800\n",
      "\tLoss: 0.328714370728. Accuracy: 86.7911111111\n",
      "  Step: 1900\n",
      "\tLoss: 0.526333332062. Accuracy: 86.7778947368\n",
      "  Step: 2000\n",
      "\tLoss: 0.355242729187. Accuracy: 86.82\n",
      "  Step: 2100\n",
      "\tLoss: 0.394891440868. Accuracy: 86.8695238095\n",
      "  Step: 2200\n",
      "\tLoss: 0.297023236752. Accuracy: 86.8836363636\n",
      "  Step: 2300\n",
      "\tLoss: 0.476174116135. Accuracy: 86.8504347826\n",
      "  Step: 2400\n",
      "\tLoss: 0.166959911585. Accuracy: 86.9008333333\n",
      "Test Loss: 0.376397259627. Test Accuracy: 86.935929483\n",
      "Epoch: 8\n",
      "  Step: 100\n",
      "\tLoss: 0.376499980688. Accuracy: 88.64\n",
      "  Step: 200\n",
      "\tLoss: 0.372916251421. Accuracy: 88.55\n",
      "  Step: 300\n",
      "\tLoss: 0.201880723238. Accuracy: 88.5666666667\n",
      "  Step: 400\n",
      "\tLoss: 0.415544509888. Accuracy: 88.255\n",
      "  Step: 500\n",
      "\tLoss: 0.207377076149. Accuracy: 88.144\n",
      "  Step: 600\n",
      "\tLoss: 0.339602708817. Accuracy: 88.2933333333\n",
      "  Step: 700\n",
      "\tLoss: 0.253129988909. Accuracy: 88.3114285714\n",
      "  Step: 800\n",
      "\tLoss: 0.286717534065. Accuracy: 88.3325\n",
      "  Step: 900\n",
      "\tLoss: 0.286336630583. Accuracy: 88.3022222222\n",
      "  Step: 1000\n",
      "\tLoss: 0.308170169592. Accuracy: 88.19\n",
      "  Step: 1100\n",
      "\tLoss: 0.34419131279. Accuracy: 88.2545454545\n",
      "  Step: 1200\n",
      "\tLoss: 0.38112449646. Accuracy: 88.1883333333\n",
      "  Step: 1300\n",
      "\tLoss: 0.475821137428. Accuracy: 88.2630769231\n",
      "  Step: 1400\n",
      "\tLoss: 0.304511189461. Accuracy: 88.29\n",
      "  Step: 1500\n",
      "\tLoss: 0.251064389944. Accuracy: 88.34\n",
      "  Step: 1600\n",
      "\tLoss: 0.410478293896. Accuracy: 88.31875\n",
      "  Step: 1700\n",
      "\tLoss: 0.29642906785. Accuracy: 88.3576470588\n",
      "  Step: 1800\n",
      "\tLoss: 0.262381732464. Accuracy: 88.4233333333\n",
      "  Step: 1900\n",
      "\tLoss: 0.184955134988. Accuracy: 88.4621052632\n",
      "  Step: 2000\n",
      "\tLoss: 0.227163940668. Accuracy: 88.432\n",
      "  Step: 2100\n",
      "\tLoss: 0.431765109301. Accuracy: 88.4257142857\n",
      "  Step: 2200\n",
      "\tLoss: 0.158089965582. Accuracy: 88.4109090909\n",
      "  Step: 2300\n",
      "\tLoss: 0.563354253769. Accuracy: 88.4234782609\n",
      "  Step: 2400\n",
      "\tLoss: 0.36588203907. Accuracy: 88.4083333333\n",
      "Test Loss: 0.388887339714. Test Accuracy: 86.3044336272\n",
      "Epoch: 9\n",
      "  Step: 100\n",
      "\tLoss: 0.167218908668. Accuracy: 90.3\n",
      "  Step: 200\n",
      "\tLoss: 0.37035804987. Accuracy: 89.76\n",
      "  Step: 300\n",
      "\tLoss: 0.391254603863. Accuracy: 89.92\n",
      "  Step: 400\n",
      "\tLoss: 0.398058056831. Accuracy: 89.895\n",
      "  Step: 500\n",
      "\tLoss: 0.15531027317. Accuracy: 89.652\n",
      "  Step: 600\n",
      "\tLoss: 0.181241735816. Accuracy: 89.6033333333\n",
      "  Step: 700\n",
      "\tLoss: 0.530353605747. Accuracy: 89.6485714286\n",
      "  Step: 800\n",
      "\tLoss: 0.267560571432. Accuracy: 89.6625\n",
      "  Step: 900\n",
      "\tLoss: 0.227786958218. Accuracy: 89.5888888889\n",
      "  Step: 1000\n",
      "\tLoss: 0.426104098558. Accuracy: 89.472\n",
      "  Step: 1100\n",
      "\tLoss: 0.398524701595. Accuracy: 89.4545454545\n",
      "  Step: 1200\n",
      "\tLoss: 0.235437676311. Accuracy: 89.46\n",
      "  Step: 1300\n",
      "\tLoss: 0.459549307823. Accuracy: 89.3953846154\n",
      "  Step: 1400\n",
      "\tLoss: 0.400467574596. Accuracy: 89.4528571429\n",
      "  Step: 1500\n",
      "\tLoss: 0.382953941822. Accuracy: 89.4546666667\n",
      "  Step: 1600\n",
      "\tLoss: 0.201712399721. Accuracy: 89.47125\n",
      "  Step: 1700\n",
      "\tLoss: 0.23145622015. Accuracy: 89.4094117647\n",
      "  Step: 1800\n",
      "\tLoss: 0.221121311188. Accuracy: 89.3811111111\n",
      "  Step: 1900\n",
      "\tLoss: 0.124913670123. Accuracy: 89.3978947368\n",
      "  Step: 2000\n",
      "\tLoss: 0.28332555294. Accuracy: 89.388\n",
      "  Step: 2100\n",
      "\tLoss: 0.276799052954. Accuracy: 89.339047619\n",
      "  Step: 2200\n",
      "\tLoss: 0.290196210146. Accuracy: 89.3290909091\n",
      "  Step: 2300\n",
      "\tLoss: 0.312798082829. Accuracy: 89.3591304348\n",
      "  Step: 2400\n",
      "\tLoss: 0.196059912443. Accuracy: 89.295\n",
      "Test Loss: 0.378131781757. Test Accuracy: 86.8438363373\n"
     ]
    }
   ],
   "source": [
    " for e in xrange(nb_epoch):\n",
    "    xi, yi = data_helpers.shuffle_matrix(xt, yt)\n",
    "    xi_test, yi_test = data_helpers.shuffle_matrix(x_test, y_test)\n",
    "    if subset:\n",
    "        batches = data_helpers.mini_batch_generator(xi[:subset], yi[:subset],\n",
    "                                                    vocab, vocab_size, check,\n",
    "                                                    maxlen,\n",
    "                                                    batch_size=batch_size)\n",
    "    else:\n",
    "        batches = data_helpers.mini_batch_generator(xi, yi, vocab, vocab_size,\n",
    "                                                    check, maxlen,\n",
    "                                                    batch_size=batch_size)\n",
    "\n",
    "    test_batches = data_helpers.mini_batch_generator(xi_test, yi_test, vocab,\n",
    "                                                     vocab_size, check, maxlen,\n",
    "                                                     batch_size=1)\n",
    "\n",
    "    accuracy = 0.0\n",
    "   \n",
    "    step = 1\n",
    "    start = datetime.datetime.now()\n",
    "    print('Epoch: {}'.format(e))\n",
    "    running_loss = 0.0\n",
    "    i=0\n",
    "    train_loss_avg=0.0\n",
    "    train_correct=0.0\n",
    "    for x_train, y_train in batches:\n",
    "        i=i+1\n",
    "        inputs=torch.from_numpy(np.swapaxes(x_train.astype(np.float64),1,2))\n",
    "        labels=torch.from_numpy(np.argmax(y_train, axis=1).astype(np.float64))\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "        inputs=inputs.float()\n",
    "        labels=labels.long()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #train_loss += loss.data[0]\n",
    "        train_loss_avg =loss.data.mean()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_correct = (predicted == labels.data).sum()\n",
    "        accuracy +=  (train_correct*100/len(labels))\n",
    "        accuracy_avg = accuracy / step\n",
    "        if step % 100 == 0:\n",
    "            print('  Step: {}'.format(step))\n",
    "            print('\\tLoss: {}. Accuracy: {}'.format(train_loss_avg,accuracy_avg))\n",
    "        step += 1\n",
    "    test_accuracy=0\n",
    "    test_loss_avg=0\n",
    "    test_correct=0\n",
    "    step=1\n",
    "    test_accuracy_avg=0\n",
    "    for x_test_batch, y_test_batch in test_batches:\n",
    "        inputs=torch.from_numpy(np.swapaxes(x_test_batch.astype(np.float64),1,2))\n",
    "        labels=torch.from_numpy(np.argmax(y_test_batch, axis=1).astype(np.float64))\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "        inputs=inputs.float()\n",
    "        labels=labels.long()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss_avg += loss.data[0]\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_correct = (predicted == labels.data).sum()\n",
    "        test_accuracy +=  (test_correct*100/len(labels))\n",
    "        #test_accuracy_avg += (test_accuracy /step)\n",
    "        step+=1\n",
    "    print('Test Loss: {}. Test Accuracy: {}'.format(test_loss_avg/step,test_accuracy/step))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=[1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
